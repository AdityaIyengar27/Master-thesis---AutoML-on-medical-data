\chapter{Conclusion and Future Work}
\label{chap:conclusion_and_future_work}
 In this chapter, the approaches and the results are summarized. Future work, improvements and open research related to this work are discussed.
 
 \section{Summary and Discussion}
 \label{sec:sumary_and_discussion}
 
The goal of the Master thesis is to develop an AutoML pipeline for atrial fibrillation (AF) and Sinus signal classification of an anonymous two-channel ECG data. Initially we concentrate on the development of seed DNN models after thoroughly exploring the data. These seed models help to determine if the convolutional models, with a restriction of the maximum number of parameters, are sufficient for the classification task. 
 
For generating new networks and exploring the search space in AutoML, the concept of Neural Architecture Search (NAS) using genetic algorithms is used. Random mutation is used to generate random networks for the initial population. For the subsequent generations, genetic mutation is used. Genetic mutation extracts the genes of a parent and depending on the mutation rate, mutates the parent's genes to produce new networks. We found out that the mutation rate, controlled by the accuracy and number of parameters of the parent, is decisive in producing good subsequent networks. 

Various selection strategies - aging, selected\_children, all\_children and lemonade are discussed and experiments are run for these strategies with different parameter settings. Aging removes the oldest network during the parent selection process for the next generation whereas selected\_children and all\_children use pairwise comparison of networks for parent selection. Lemonade uses the concept of Pareto-front for the parent selection process. All these strategies produce good networks with the restriction of the maximum number of parameters allowed. All\_children produces 35\% networks with less than 10,000 parameters with detection rate greater than 90\% and false alarm rate less than 20\%, which is less than one fourth over the seed models with respect to number of parameters. Thus, these selection strategies using multiple metrics along with the mutation policies for NAS have proven to improve over the seed models and produce very good results for the classification of atrial fibrillation.

\section{Future work}
\label{sec:future_work}

There is enormous potential for AutoML for such problems. Additional techniques for further reducing model size include quantization. Quantization is a concept that refers to the process of reducing the number of bits that are used to represent a number. This essentially means that quantization can be used to reduce bandwidth and storage as well as increase arithmetic operation performance. In the context of neural networks and deep learning, the predominant format used for weights and biases has been 32-bit floating-point or FP32. INT8 for weights, activations and biases consumes 4x less overall bandwidth compared to FP32. Additionally, integer computations are faster compared to floating-point computations and the hardware will consume less area and is also energy efficient.

Quantization can be part of the AutoML pipeline to determine the format that can be used without compromising the accuracy. This also plays out for hardware-specific implementations of neural networks. For example, FPGAs play out its strength for quantization as they are not limited to standard floating-point precision like FP32 format or FP16 formats, but can be used to represent any number of bits - such as 6-bit values. To improve accuracy, certain fine-tuning can be used. One such method is called scale factor, which can be used to adapt the dynamic range of the tensor from FP32 format to an integer format. This scale factor can be calculated per-layer but can be further used per-tensor as well.

Additional hardware constraints to further fine-tune the models, such as number of floating point operations (FLOPS), memory consumption and inference time of the models can also be added to the search space. 

Another research direction would be to use cell based approach for creating and exploring the search space. For this approach, the architecture for the networks can consists of multiple cells. Each cell would in turn consists of either one or more convolutional layer (with different filter size, kernel size and stride size) along with a pooling layer. Other layers such as sum and concatenation layers can also be part of the cell. Thus, each cell will have its own search space, which would add more flexibility to the process. The cell based genetic approach can then be compared the differentiable architecture search \cite{liu2018darts}. Reinforcement Learning is another such technique which can be used to compare with the current methods. As such, there is much more to be explored.
