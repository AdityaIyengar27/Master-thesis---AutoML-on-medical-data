\chapter{Related Work and state of the art} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{chap:related_work}

\section{Electrocardiogram Signal Classification for Atrial Fibrillation Detection using Deep Neural Network}
\label{sec:ecg_using_DNN}

Electrocardiogram (ECG) has become a fundamental tool for diagnosing a wide variety of arrhythmic abnormalities. Computer-aided interpretation of the ECG data has become increasingly important to serve as a crucial adjunct to clinical interpretation. Significant improvement in deep neural network algorithms to learn abstract, high-level representations of the medical data has also contributed to improvements in ECG interpretation. 

There have been many contributions for developing DNN for ECG data. The 2017 PhysioNet/CinC Challenge \cite{clifford2017af} had encouraged participants to classify sinus rhythm (SN) or atrial fibrillation (AF), an alternative rhythm from a single short ECG lead recording (between 30 seconds and 60 seconds in length). This had led the participants to try and develop various neural network models for classification. A simple 16-layered convolutional neural network, where each layer would contain multiple sub layers - batch normalization, ReLU activation, dropouts, 1D convolution, and 1D average pool was developed by researchers in New Zealand \cite{xiong2017robust}. Their novel neural architecture also contained many skip connections and demonstrated CNNs can perform efficient classification. A similar 16-layered CNN architecture was proposed by Yildirim, O. et al. \cite{yildirim2018arrhythmia} for multi-class classification which was fast and efficient with high accuracy (91.33 \%). Another architecture that contained long short term memory (LSTM) units along with CNN layers to account for feature aggregation across time was proposed in this challenge \cite{zihlmann2017convolutional} which paved the way to use of temporal analysis as well. 

In more recent research \cite{hannun2019cardiologist}, the authors create a novel ECG dataset that underwent an expert annotation. They develop a comprehensive end to end DNN approach to perform the classification across a broad range of the most common and important ECG rhythm diagnoses. The 34-layered DNN was developed to accomplish the tasks of the entire ECG pipeline, that is feature extraction, feature selection/reduction, and classification. Their research also showed that not much preprocessing, such as Fourier or wavelet transforms, is required on the ECG data to achieve strong classification performance. Their findings demonstrate that with sufficient training data, using a DNN in such an end to end approach has the potential to improve the accuracy and prediction performance of interpretation of ECG data.

\section{Deep Neural Network for specialized hardware}
\label{sec:DNN_for_specialized_hardware}

Recent works in the area of CNNs have produced state of the art neural networks models for image recognition, speech recognition, and time-series data. These models have improved the accuracy for both image recognition tasks and classification, but also have improved the predictions for time series data. There have also been research projects in recent years not only to improve neural network's performance on CPUs but also to develop specialized hardware such as GPUs, FPGAs, ASICs, etc for training neural networks to improve the network's performance in terms of training time and efficiency.

Vanhoucke et al. of Google \cite{vanhoucke2011improving} suggested methods and tools to improve the performance of the neural networks on CPUs. For floating-point implementation, the idea to localize the data available to the CPU by distributing it to nearby memory locations would increase the CPU performance. Methods like loop unrolling, parallel accumulators which gives the compiler more freedom in pipe-lining operations, data layout optimization for Single Instruction Multiple Data(SIMD) instructions for low-level parallelization on CPUs improves CPU's performance. Despite this, current GPUs have better performance.

There has previous work done by researchers such as Pyakillya et al. on ECG classification by using GPU for training neural networks \cite{pyakillya2017deep}. Their work presented the idea that ECG classification results can be obtained by 1D convolutional networks with fully connected network (FCN) layers on time-series data with decent accuracy on the validation dataset. Alfaras et al. proposed a heartbeat classifier on Echo State Network (ESN) \cite{alfaras2019fast} which also showed that computation times of a parallel computing architecture such as GPU outperforms a CPU in the classification of heartbeats using ECG data.

The other type of specialized hardware used is the FPGAs. Microsoft announced a project in 2014 that demonstrated that FPGAs can accelerate Bing Ranking by a factor of nearly 2x in their datacenters \cite{ovtcharov2015accelerating}. The researchers leveraged the infrastructure of the FPGAs to develop a high throughput CNN-FPGA accelerator yielding a high-performance system that consumed considerably less power. 

In more recent works by Nurvitadhi et al. \cite{nurvitadhi2017can}, the authors evaluated the emerging DNN algorithms such as ResNet, VGG, etc. on various generations of GPUs (NVIDIA Titan X Pascal) and FPGAs (i.e., Intel Arria 10 GX 1150 and Intel Stratix 10 2800). The experiments show that FPGAs are extremely customizable and the current trends in DNN favour the FPGA platform as they offer superior energy efficiency. 

Shawahna et al. talk about the challenges of FPGA-based implementation of deep learning networks \cite{shawahna2018fpga}. CNNs have a requirement of a significant amount of storage, external bandwidth, and computational resources. For example, AlexNet CNN has over 60 million model parameters that need 250MB of memory for storing the weights based on 32-bit floating-point representation and this large amount of storage is not supported by FPGAs. Also, various CNN layers have different implementations, and hence it becomes important to carefully design the architecture of CNN to maximize the performance of CNN layers. It is not always feasible to design such architectures and hence, the AutoML/NAS concepts that can help design architectures for FPGAs can be used.

\section{AutoML/Neural Architecture Search}
\label{sec:autoML_NAS}

As mentioned in the previous section, memory constraints on FPGAs play a big factor in its configuration. Fedorov et al. discuss about sparse architecture search for CNNs on the micro-controller unit (MCU) in their paper \cite{NIPS2019_8743}. Deployment of CNNs on MCUs is challenging as MCUs are highly resource-constrained. The authors use Sparse Architecture search (SpArSe) combining neural architecture search (NAS) and network pruning in conjunction with hyperparameters around morphology and training to discover models that are highly performant with memory-constrained architectures (i.e. models with fewer parameters).

In the paper \cite{sun2020automatically}, the authors talk about the framework for a genetic algorithm which is easy to understand and easily extendable for images. The authors also talk about using various selection strategies for mutation along with the creation of initial networks for evaluation while performing NAS. Liu et al. propose a method for efficient architecture search in their paper, called DARTS (Differentiable ARchiTecture Search) \cite{liu2018darts}. The authors propose to relax the search space of NAS to be continuous so that the architecture can be optimized for its validation set performance by gradient descent. 

In another research, \cite{lu2018nsga}, the authors talk about a multi-objective evolutionary approach for neural architecture search. The NSGA-Net, as the authors explain, can effectively optimize and trade-off multiple objective functions which is also the possibility while designing NAS for FPGAs. The paper also mentions the importance of an efficient exploration and exploitation strategy of search space for the optimization of networks while using NAS. 

There has been research about using NAS for medical data. Rapaport et al. in \cite{rapaport2019eegnas} talk about using neural architecture search for Electroencephalography (EEG) Data Analysis and Decoding. EEG is the acquisition and decoding of electric brain signals which is similar to ECG, except for the electrical signals are from the brain. The authors employ genetic algorithms (GAs) as the means for NAS and analyze the evolution of NN architectures over time, which helps better understand the NAS process. The authors concluded that the ability to generalize in EEG classiÔ¨Åcation is key to good CNN  architectures.

\afterpage{\blankpage}
% \section{Quantization}
% \label{sec:quantization}

% Deep neural networks can be memory intensive with the weights represented as floating point values. This makes the deployment of DNNs on memory constrained hardwares such as FPGAs difficult. Han et al. showed that trained quantization can reduce the number of bits required to represent each weight \cite{han2015deep}, by storing a index into a table of shared weights without affecting accuracy.

% Other works by Lin, D. Darryl et al. presented a fixed point implementation of DNNs which also reduced memory bandwidth, lower power consumption and less computation time \cite{lin2016fixed} without any loss of accuracy. Loroch et al. \cite{loroch2017tensorquant} developed a toolbox for Tensorflow to analyse the impact of layer-wise quantization. The authors analysed the impact of rounding of the weights on the DNNs accuracy by emulating fixed point operations. They also analyse the impact of intrinsic and extrinsic quantization. In the latter, quantization is applied at the end of the layer where as in the former, quantization is applied within a layer on every arithmetic operation. Thus, in this paper quantization is emulated as close as possible to a custom hardware such as FPGAs.